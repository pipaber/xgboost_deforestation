# Peru Deforestation Modeling (District-Year)
XGBoost model + SHAP explainability + scenario analysis + 2021–2024 observed-loss validation + API

This repository builds and evaluates a district-year deforestation prediction pipeline for Peru. It includes:

- **Training**: XGBoost regression trained on `log1p(Def_ha)` (predicts hectares after `expm1`).
- **Explainability**: SHAP plots for global and local interpretation.
- **Scenario analysis**: deterministic “good/mild/bad” perturbations of drivers.
- **Mapping**: Peru district bubble maps (admin-3 bubbles; admin-2 boundaries) generated by scenario runs.
- **Observed forest loss**: curated district loss (2001–2024) and remaining forest 2024.
- **Hindcast 2021–2024**: scenario-based evaluation comparing model projections vs observed loss.
- **Quarto report**: renders a single HTML report including metrics and all images.
- **API**: serves deforestation predictions + grouped SHAP contribution percentages for decision support.

---

## Repository layout (high level)

### Data (in repo root)
- `deforestation_dataset_PERU_imputed_coca.csv` (training/inference dataset; `;` separated)
- `Bosque_y_perdida_de_bosques_por_Distrito_al_2024_curated.csv`
  - district forest loss by year (2001–2024, `*_ha`)
  - remaining forest: `BOSQUE AL 2024_ha`
- `DISTRITOS_inei_geogpsperu_suyopomalia.zip` (district geometry; used for mapping)
- `Bosque_NoBosque_Perdida_2001_2024_Raster/` (raster assets; optional)

### Models
- `models/xgb_timecv_v1/`
  - `bundle.joblib`
  - `feature_columns.json`
  - `model.json`
  - `metrics_report.json`
  - `trials_log.csv`
  - `test_predictions.csv`

### Reports
- `reports/shap/<run>/` — SHAP summary + dependence + waterfall plots
- `reports/scenarios/<run>/` — per-scenario CSVs and plots (including maps)
- `reports/forest_loss_trends/` — observed-loss trend plots (dept/province, 2021–2024 focus)
- `reports/hindcast_2021_2024/` — observed vs predicted evaluation for 2021–2024
- `reports/quarto/deforestation_report.qmd` — Quarto report source (renders HTML)

### Source code (scripts)
- `src/deforestation/train_xgb_timecv.py` — rolling/expanding window time CV tuning (recommended)
- `src/deforestation/explain_shap.py` — generate SHAP plots
- `src/deforestation/run_scenarios.py` — baseline-year scenarios + plots + bubble maps
- `src/deforestation/analysis/forest_loss_trends.py` — plots observed loss trends from curated CSV
- `src/deforestation/analysis/hindcast_2021_2024.py` — scenario-based hindcast vs observed 2021–2024
- `src/deforestation/api.py` — prediction API with grouped SHAP contributions

---

## Environment / dependencies

This project uses `uv` for Python dependency management (`pyproject.toml` + `uv.lock`).

### Install / run
- Install deps:
  - `uv sync` (or `uv add <package>` if missing)
- Run scripts:
  - `uv run python <script> ...`

### SHAP compatibility
SHAP uses `numba`, which requires **NumPy <= 2.3.x**. If you see:

> ImportError: Numba needs NumPy 2.3 or less

Fix with:
- `uv add "numpy<2.4"`

---

## Model definition

### Target
- `Def_ha` = deforestation area (hectares)

### Target transform
Training is done in log-space:
- `y_train = log1p(Def_ha)`
Predictions are returned in hectares:
- `pred_ha = expm1(pred_log)`

---

## Training (recommended): rolling/expanding time CV

Script:
- `src/deforestation/train_xgb_timecv.py`

Why:
- avoids leakage and “overfitting a single validation window”
- selects hyperparameters that generalize across multiple future-like folds

Example (CPU):
```
uv run python src/deforestation/train_xgb_timecv.py \
  --data deforestation_dataset_PERU_imputed_coca.csv \
  --sep ';' \
  --trials 200 \
  --seed 42 \
  --outdir models \
  --run-name xgb_timecv_v1 \
  --device cpu
```

Artifacts saved under `models/<run-name>/`:
- `bundle.joblib` (model + schema + config)
- `feature_columns.json`
- `metrics_report.json`
- `model.json`
- `trials_log.csv`
- `test_predictions.csv`

---

## Explainability (SHAP)

Script:
- `src/deforestation/explain_shap.py`

Example:
```
uv run python src/deforestation/explain_shap.py \
  --bundle models/xgb_timecv_v1/bundle.joblib \
  --data deforestation_dataset_PERU_imputed_coca.csv \
  --sep ';' \
  --split test \
  --out reports/shap/xgb_timecv_v1 \
  --topk 15
```

Outputs (under `reports/shap/<run>/`):
- `shap_summary_beeswarm.png`
- `shap_summary_bar.png`
- `shap_dependence_*.png`
- `shap_waterfall_example.png`
- `shap_mean_abs.csv`
- `top_features.txt`
- `shap_run_metadata.json`

How to interpret:
- Beeswarm: direction + distribution of effects (per row)
- Bar: global importance by mean(|SHAP|)
- Waterfall: local explanation for one example row
- Dependence: nonlinear relationships and interaction hints

---

## Scenario analysis + mapping

Script:
- `src/deforestation/run_scenarios.py`

Scenario config:
- `scenarios/scenarios.yaml`

What it does:
- selects a baseline slice (default `YEAR=2020`)
- applies deterministic transforms (good/mild/bad)
- predicts baseline and scenario
- computes deltas
- writes per-district and aggregated outputs
- generates plots and bubble maps

Example:
```
uv run python src/deforestation/run_scenarios.py \
  --bundle models/xgb_timecv_v1/bundle.joblib \
  --data deforestation_dataset_PERU_imputed_coca.csv \
  --sep ';' \
  --scenarios scenarios/scenarios.yaml \
  --out reports/scenarios/xgb_timecv_v1_baseline2020
```

Outputs under `reports/scenarios/<run>/`:
- `scenario_summary.json`
- `scenarios_total_pred_ha.png`
- `scenarios_total_delta_ha.png`
- per scenario folder (e.g., `good/`, `mild/`, `bad/`):
  - `*_district_results.csv`
  - `*_by_region.csv`, `*_by_department.csv`
  - `*_delta_hist.png`
  - `*_delta_by_region.png`
  - `*_top20_delta.png`
  - `*_bubblemap_pred_size_delta_color.png`
    - bubbles at district centroids
    - bubble size = predicted deforestation (ha)
    - bubble color = delta vs baseline (ha)
    - admin-2 boundaries derived from district polygons

---

## Observed forest loss (2001–2024): trend plots

Observed loss input:
- `Bosque_y_perdida_de_bosques_por_Distrito_al_2024_curated.csv`

Script:
- `src/deforestation/analysis/forest_loss_trends.py`

Run:
```
uv run python src/deforestation/analysis/forest_loss_trends.py \
  --csv Bosque_y_perdida_de_bosques_por_Distrito_al_2024_curated.csv \
  --out reports/forest_loss_trends \
  --top-provinces 20
```

Produces:
- department trends (2001–2024 and 2021–2024)
- province top-N trends
- remaining forest 2024 plots
- long-format CSVs for reuse

---

## Hindcast evaluation (2021–2024): predicted vs observed

Goal:
Compare observed forest loss (2021–2024) against model “projected” deforestation using:
- 2020 covariate baseline
- macro/scenario assumptions for missing 2021–2024 covariates
- official department coca totals (2020–2024)

Observed loss input:
- `Bosque_y_perdida_de_bosques_por_Distrito_al_2024_curated.csv`

Coca totals input (department totals 2020–2024):
- `data_external/coca_department_2020_2024.csv`

Script:
- `src/deforestation/analysis/hindcast_2021_2024.py`

Assumptions currently supported (transparent; editable):
- temperature delta based on NOAA global anomaly, with baseline 2020 = 1.02°C
- precipitation multiplicative factors (scenario-like) + region scaling
- population growth applied uniformly to district population features
- small multipliers for mining/infrastructure/agriculture (political uncertainty assumption)
- coca scaled to match official department totals 2020–2024 (district shares preserved)

Run:
```
uv run python src/deforestation/analysis/hindcast_2021_2024.py \
  --bundle models/xgb_timecv_v1/bundle.joblib \
  --data deforestation_dataset_PERU_imputed_coca.csv \
  --sep ';' \
  --loss Bosque_y_perdida_de_bosques_por_Distrito_al_2024_curated.csv \
  --out reports/hindcast_2021_2024 \
  --coca-dept-csv data_external/coca_department_2020_2024.csv
```

Outputs under `reports/hindcast_2021_2024/`:
- `observed_vs_pred_district_2021_2024.csv`
- `observed_vs_pred_by_department_2021_2024.csv`
- `observed_vs_pred_by_province_2021_2024.csv`
- `metrics_by_year.csv`
- `metrics_by_department.csv`
- plots:
  - `by_department_trends_observed_vs_pred.png`
  - `by_province_topN_trends_observed_vs_pred.png`
  - `scatter_observed_vs_pred_2021_2024.png`
  - `residuals_hist.png`
- `assumptions_used.json` (critical for reproducibility and transparency)

Interpretation:
This is a **scenario-based hindcast**, not a strict forecast, unless you provide true district-year covariates for 2021–2024. The goal is to test whether the model’s structure can track post-2020 observed trends under plausible covariate trajectories.

---

## Quarto report

Source:
- `reports/quarto/deforestation_report.qmd`

Render:
```
quarto render reports/quarto/deforestation_report.qmd
```

The report includes:
- training/test metrics (from `models/.../metrics_report.json`)
- all SHAP images (galleries)
- all scenario images (galleries)
- trend and hindcast plots (if referenced/added)

---

## API (prediction + grouped SHAP “percent contributions”)

Script:
- `src/deforestation/api.py`

Purpose:
Serve deforestation predictions (ha) plus “percent contribution” by driver group based on SHAP:

- Mining
- Infrastructure
- Agriculture
- Climate
- Socioeconomic
- Geography/Admin
- Other

Percent contribution definition:
- `pct(group) = sum(|SHAP features in group|) / sum(|SHAP all features|)`

This represents **share of model reasoning**, not causal attribution.

### Run the API
From repo root:
```
uv run uvicorn deforestation.api:app --host 0.0.0.0 --port 8000
```

### Health check
```
curl http://localhost:8000/health
```

### Predict with baseline defaults + overrides (modeled “hindcast-style” usage)
The API is designed so the user does NOT need to provide every feature. The server:
- loads the baseline row for `UBIGEO` at `DEFORESTATION_BASELINE_YEAR` (default 2020)
- applies your numeric overrides (e.g., set YEAR=2024 and adjust Minería, Pobreza, pp, tmean, etc.)
- predicts hectares
- returns grouped SHAP contributions

Example request (predict year 2024 with overrides):
```
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "ubigeo": "010101",
    "overrides": {
      "YEAR": 2024,
      "Minería": 1.10,
      "Infraestructura": 1.02,
      "area_agropec": 1.01,
      "pp": 0.97,
      "tmean": 0.26
    },
    "include_contributions": true
  }'
```

Notes:
- `ubigeo` identifies the district baseline feature row (baseline year).
- `YEAR` in overrides lets you “forecast” beyond 2020 as a scenario input.
- Omitted variables default to baseline values.
- The response includes:
  - `predictions_ha`: predicted deforestation in hectares
  - `driver_contributions`: grouped SHAP percentages
  - `meta` including which overrides were applied

### API vs hindcast scripts
- The **hindcast scripts** are batch evaluation comparing model vs observed 2021–2024 across all districts.
- The **API** is for interactive, per-district “what-if” predictions with explainability.

If you want the API to exactly mirror the hindcast’s macro assumptions (temperature anomaly series, population growth, coca department scaling), it can be extended to load those assumptions server-side and apply them automatically. The current API is baseline+overrides (general scenario tool), while hindcast is a standardized batch evaluation.

---

## Reproducibility checklist
When publishing results, record:
- bundle path (`models/.../bundle.joblib`)
- dataset version and separator
- scenario config (`scenarios/scenarios.yaml`)
- hindcast assumptions (`reports/hindcast_2021_2024/assumptions_used.json`)
- git commit hash

---

## GitHub publishing notes
Recommended to NOT commit:
- `.venv/` (large)
- Quarto render caches / intermediates (`reports/quarto/*_files`, etc.)

This repo includes a `.gitignore` that ignores the above by default.

If you include large geodata (shapefiles/rasters), consider Git LFS.
